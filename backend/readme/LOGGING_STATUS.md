# RAG Evaluation Logging Status

## ‚úÖ Currently Logging:

### 1. Question / Response from RAG Agent ‚úÖ
- **Query**: Full user question
- **Response**: RAG system's generated response
- **Response Time**: How long it took to generate
- **Timestamp**: When evaluation occurred

### 2. Retrieved Documents ‚úÖ **FULLY IMPLEMENTED - NO REDUNDANCY**
- **Document Contents**: Actual retrieved documents from RAG system execution
- **Document Metadata**: Source, content length, content preview
- **Tool Execution Details**: Tool call IDs, response lengths, tool names
- **Extraction Method**: How documents were captured (from actual tool messages)
- **No Redundant Calls**: Documents captured from the same execution that generates the response

### 3. Judges Individual Assessment per Criteria ‚úÖ
For each evaluation criterion, we log:
- **Individual LLM Scores**: Each judge's score (0-10)
- **Individual LLM Reasoning**: Each judge's detailed reasoning
- **Individual LLM Confidence**: Each judge's confidence level (0-1)
- **Provider/Model Info**: Which LLM gave which score
- **Final Consensus Score**: Aggregated final score
- **Final Consensus Reasoning**: Combined reasoning

## üìÅ Log File Structure:
```
logs/rag_evaluation/
‚îú‚îÄ‚îÄ json/                    # Complete evaluation data (JSON)
‚îú‚îÄ‚îÄ summaries/              # Human-readable summaries
‚îú‚îÄ‚îÄ llm_scores/            # Detailed LLM scoring breakdown with tool execution details
‚îî‚îÄ‚îÄ debug/                 # Debug logs and process info
```

## üìä What Each Log Contains:

### JSON Logs (`json/` folder):
```json
{
  "query": "What are PTSD symptoms?",
  "response": "PTSD symptoms include...",
  "context_documents": ["Doc 1 content", "Doc 2 content"],
  "retrieved_documents_metadata": [
    {
      "document_index": 0,
      "source": "ptsd_guide.pdf",
      "content_length": 1250,
      "content_preview": "PTSD affects approximately...",
      "tool_call_id": "call_abc123",
      "extraction_method": "from_tool_message"
    }
  ],
  "tool_execution_details": [
    {
      "tool_name": "retrieve_information",
      "tool_call_id": "call_abc123",
      "response_length": 2847,
      "response_preview": "Relevant information found:\n\n**Source 1** (from: ptsd_guide.pdf)..."
    }
  ],
  "evaluation_report": {
    "overall_score": 8.5,
    "evaluation_results": {
      "retrieval_relevance": {
        "score": 9.0,
        "reasoning": "Documents are highly relevant...",
        "confidence": 0.95,
        "individual_scores": [...]
      }
    }
  },
  "response_time": 1.23,
  "timestamp": "2024-03-14T15:30:45"
}
```

### LLM Scores Logs (`llm_scores/` folder):
```
QUERY 1: What are PTSD symptoms?
================================

RAG RESPONSE:
PTSD symptoms include intrusive memories, avoidance behaviors...

TOOL EXECUTION DETAILS:
Tool 1: retrieve_information
  Call ID: call_abc123
  Response Length: 2847 characters
  Response Preview: Relevant information found:

**Source 1** (from: ptsd_guide.pdf): PTSD affects approximately...

RETRIEVED DOCUMENTS (from actual RAG execution):
Document 1:
  Source: ptsd_guide.pdf
  Length: 1250 characters
  Extraction Method: from_tool_message
  Preview: PTSD affects approximately 11-20% of veterans...

Document 2:
  Source: mental_health_handbook.pdf
  Length: 890 characters
  Extraction Method: from_tool_message
  Preview: Symptoms of PTSD can be grouped into four categories...

CONTEXT DOCUMENTS USED FOR EVALUATION:
Context 1: PTSD affects approximately 11-20% of veterans who served...
Context 2: Symptoms of PTSD can be grouped into four categories...

Retrieval Relevance:
Final Score: 9.0/10
Confidence: 0.95
Reasoning: Documents are highly relevant...

Individual LLM Scores:
- chatopenai (meta-llama/Llama-3.3-70B-Instruct-Turbo-Free):
  Score: 9.0/10
  Confidence: 0.98
  Reasoning: Excellent relevance to PTSD symptoms...
```

## ‚úÖ Implementation Details:

### How Retrieved Documents are Captured (NO REDUNDANCY):
1. **Single RAG Execution**: The pipeline runs `rag_system.graph.invoke(initial_state)` once
2. **Message Capture**: Captures all messages from the graph execution, including ToolMessages
3. **Document Extraction**: Parses the actual tool responses to extract retrieved documents
4. **Metadata Extraction**: Extracts source information, content length, and previews from tool messages
5. **No Separate Retrieval**: No additional calls to the retriever - uses the same documents that generated the response

### Integration with RAG Agent:
- **RAG Agent Flow**: Query ‚Üí LLM decides to call tool ‚Üí Retrieval Tool ‚Üí Retrieved Documents ‚Üí LLM Response
- **Evaluation Flow**: Query ‚Üí Capture entire RAG execution ‚Üí Extract documents from tool messages ‚Üí Evaluate
- **Perfect Alignment**: The documents used for evaluation are exactly the same ones used to generate the response
- **Tool Traceability**: Full visibility into which tools were called and their responses

### Key Improvements:
- **Eliminated Redundancy**: No separate retrieval calls
- **Perfect Accuracy**: Documents used for evaluation are exactly what the RAG system used
- **Tool Transparency**: Full visibility into tool execution
- **Extraction Method Tracking**: Know exactly how documents were captured

## ‚úÖ Summary:
- **Question/Response**: ‚úÖ Fully logged
- **Retrieved Documents**: ‚úÖ Fully implemented with NO redundancy
- **Individual Judge Assessments**: ‚úÖ Fully logged with detailed breakdown

## üéØ What You Can Now Analyze:
1. **Retrieval Quality**: See exactly which documents were retrieved and used
2. **Tool Performance**: Monitor tool execution times and response sizes
3. **Document Relevance**: Evaluate if the right documents were found and used
4. **Response Quality**: Compare the response against the actual retrieved content
5. **Judge Consistency**: See how different LLMs scored the same retrieval/response
6. **Source Attribution**: Track which sources contributed to each response
7. **Tool Call Traceability**: Full audit trail of tool executions
8. **Execution Efficiency**: Monitor the complete RAG pipeline performance

## üöÄ Benefits of This Approach:
- **No Redundant API Calls**: More efficient and cost-effective
- **Perfect Data Integrity**: Evaluation uses exactly the same data as response generation
- **Complete Transparency**: Full visibility into the RAG execution process
- **Accurate Evaluation**: No discrepancies between retrieved docs and evaluation docs 